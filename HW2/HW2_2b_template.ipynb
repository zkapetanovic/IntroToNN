{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "import load_cifar as data\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Hyper-perparmeter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10;\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "\n",
    "# Probability for how many units are kep in each layer (dropout technique)\n",
    "dr = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='output')\n",
    "d = tf.placeholder(tf.float32, name=\"droput_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Neural Network Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x, dropout_rate):\n",
    "\n",
    "    ### Define convolutional layers ###\n",
    "\n",
    "    # 64 filters, 3x3\n",
    "    filt1 = tf.Variable(tf.truncated_normal(shape=[3,3,3,64], mean=0, stddev=0.08))\n",
    "    filt2 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    filt3 = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    \n",
    "    # Convolution/Pooling layers\n",
    "    c1 = tf.nn.conv2d(x, filt1, strides=[1,1,1,1], padding='SAME')\n",
    "    c1 = tf.nn.relu(c1)\n",
    "    p1 = tf.nn.max_pool(c1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    cn1 = tf.layers.batch_normalization(p1)\n",
    "    \n",
    "    c2 = tf.nn.conv2d(cn1, filt2, strides=[1,1,1,1], padding='SAME')\n",
    "    c2 = tf.nn.relu(c2)\n",
    "    p2 = tf.nn.max_pool(c2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    cn2 = tf.layers.batch_normalization(p2)\n",
    "    \n",
    "    #c3 = tf.nn.conv2d(c2, filt3, strides=[1,1,1,1], padding='SAME')\n",
    "    #c3 = tf.nn.relu(c3)\n",
    "    #p3 = tf.nn.max_pool(c3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    #cn3 = tf.layers.batch_normalization(p3)\n",
    "\n",
    "\n",
    "    ### Define fully connected layers ###\n",
    "\n",
    "    # Flatten the output\n",
    "    flat = tf.contrib.layers.flatten(cn2)\n",
    "\n",
    "    # Fully connected layer (128)\n",
    "    f1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    f1 = tf.nn.dropout(f1, dr)\n",
    "    f1 = tf.layers.batch_normalization(f1)\n",
    "    \n",
    "    out = tf.contrib.layers.fully_connected(inputs=f1, num_outputs=10, activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = neural_net(x, dr)\n",
    "\n",
    "# Define cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "# Define accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and testing</h1>\n",
    "<h2>1.Print out validation accuracy after each training poch</h2>\n",
    "<h2>2.Print out training time you spend on each epoch</h2>\n",
    "<h2>3.Print out testing accuracy in the end</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_neural_net(session, optimizer, dr, features, labels):\n",
    "    session.run(optimizer,\n",
    "                feed_dict={\n",
    "                    x: features,\n",
    "                    y: labels,\n",
    "                    d: dr\n",
    "                })\n",
    "    \n",
    "def get_stats(session, cost, accuracy, b_feat, b_labels, v_feat, v_labels):\n",
    "    loss = sess.run(cost,\n",
    "                   feed_dict={\n",
    "                       x: b_feat,\n",
    "                       y: b_labels, \n",
    "                       d: 1.\n",
    "                   })\n",
    "    \n",
    "    acc = sess.run(accuracy, \n",
    "                  feed_dict={\n",
    "                      x: v_feat,\n",
    "                      y: v_labels,\n",
    "                      d: 1.\n",
    "                  })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2067 Validation Accuracy: 0.211200\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.7165 Validation Accuracy: 0.362200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.3938 Validation Accuracy: 0.435000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.3542 Validation Accuracy: 0.486000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.3098 Validation Accuracy: 0.505200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.4718 Validation Accuracy: 0.516000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.1552 Validation Accuracy: 0.573200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     0.8672 Validation Accuracy: 0.570200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     0.8425 Validation Accuracy: 0.594200\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.0920 Validation Accuracy: 0.612200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.1369 Validation Accuracy: 0.596400\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.8816 Validation Accuracy: 0.631400\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.6874 Validation Accuracy: 0.640200\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.6629 Validation Accuracy: 0.644200\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.7720 Validation Accuracy: 0.648600\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.8184 Validation Accuracy: 0.655800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.6580 Validation Accuracy: 0.662000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.3943 Validation Accuracy: 0.648800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.4403 Validation Accuracy: 0.670000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.6661 Validation Accuracy: 0.668600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.5932 Validation Accuracy: 0.662400\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.4864 Validation Accuracy: 0.669000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.4098 Validation Accuracy: 0.668400\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.3854 Validation Accuracy: 0.674600\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.4718 Validation Accuracy: 0.678800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.5748 Validation Accuracy: 0.660400\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.3598 Validation Accuracy: 0.675000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.2525 Validation Accuracy: 0.685800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.2761 Validation Accuracy: 0.668600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.2590 Validation Accuracy: 0.683800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.3631 Validation Accuracy: 0.678800\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.3507 Validation Accuracy: 0.675800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.2225 Validation Accuracy: 0.675000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.2678 Validation Accuracy: 0.657400\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.2351 Validation Accuracy: 0.667400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.2060 Validation Accuracy: 0.663400\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.1525 Validation Accuracy: 0.686000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.1217 Validation Accuracy: 0.679000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.1878 Validation Accuracy: 0.683000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.2069 Validation Accuracy: 0.672000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.2088 Validation Accuracy: 0.681800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.1443 Validation Accuracy: 0.666000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.1038 Validation Accuracy: 0.678200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.1722 Validation Accuracy: 0.679800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.1824 Validation Accuracy: 0.666600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.2059 Validation Accuracy: 0.669600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.1175 Validation Accuracy: 0.673200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.1025 Validation Accuracy: 0.671000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.0954 Validation Accuracy: 0.685400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.1143 Validation Accuracy: 0.681400\n"
     ]
    }
   ],
   "source": [
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "v_feat, v_labels = pickle.load(open('preprocessed_validation.p', mode='rb'))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches+1):\n",
    "            for batch_features, batch_labels in data.load_preprocessed_training_batch(batch_i, batch_size):\n",
    "                train_neural_net(sess, optimizer, dr, batch_features, batch_labels)\n",
    "\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            get_stats(sess, cost, accuracy, batch_features, batch_labels, v_feat, v_labels)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Discussion</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15px\">\n",
    "I acheived the above results, by slowly adding complextity to the neural network architecture to exceed 50% accuracy. I started off with just a fully connected network (10 nodes) and the accuracy was very low. Then I added a convolutional and pooling layer before the fully connected layer. This acheived an accuracy of around 30%. I also lowered the learning rate from 0.1 to 0.001 </p>\n",
    "<p style=\"font-size:15px\">\n",
    "Next, I tried adding more convolution + pooling layers, but the accuracy got worse. I went back to just two convolution + pooling layers and then I added a fully connected layer with 128 nodes before my final fully connected layer of 10 nodes and I was able to achieve 68% accuracy within 10 epochs. In fact, 50% accuracy was reached in the first epoch. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
